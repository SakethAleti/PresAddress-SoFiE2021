{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual env libraries\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../../../.local/lib/python3.9/site-packages')))\n",
    "sys.path.append(os.path.abspath(os.path.join('../../../virtualenv/lib/python3.9/site-packages')))\n",
    "sys.path.append('/usr/local/sas/grid/python3-3.9.1/lib/python3.9/site-packages')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import saspy\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import seaborn as sns\n",
    "import wrds\n",
    "import glob\n",
    "from stdnum import cusip\n",
    "from pandas.tseries.offsets import MonthEnd, MonthBegin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "* Set up SAS and SQL Sessions\n",
    "* Prepare list of stocks for data collection\n",
    "* Prepare list of dates for data collection\n",
    "* For each date, grab data for the list of stocks\n",
    "* Save daily hfprices to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up WRDS sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# sas = saspy.SASsession(**{'cfgname': 'default', 'encoding': 'utf_8'})\n",
    "sql = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_info_df = pd.read_feather('../../../HFZoo/data/keys/stock_universe.feather').set_index('dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CRSP Daily List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_folder = '../../../HFZoo/data/crsp/daily/'\n",
    "crsp_filenames = glob.glob(crsp_folder + '*.parquet')\n",
    "crsp_dates = [x.split('/')[-1].split('.')[0] for x in crsp_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dates List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dates for each library\n",
    "taqm_dates = list(pd.read_csv('../../data/taq/taqm_dates_list.csv').iloc[:,0])\n",
    "taqmsec_dates = list(pd.read_csv('../../data/taq/taqmsec_dates_list.csv').iloc[:,0])\n",
    "\n",
    "## Merge and filter by yyyymm input\n",
    "taq_all_dates = taqm_dates + taqmsec_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help convert python args to SAS code\n",
    "list_to_str = lambda lst: ', '.join([\"'\" + x + \"'\" for x in lst])\n",
    "\n",
    "# Modify SAS scripts with given args\n",
    "def gen_sas_script(base_script, date_ymd, start_time, end_time, sample_interval_s, output_filename, permno_selects, cusip_selects, symbol_selects):\n",
    "\n",
    "    base_script[10] = f\"%let start_time_m = '{start_time}'t;\\n\"\n",
    "    base_script[11] = f\"%let end_time_m = '{end_time}'t;\\n\"\n",
    "    base_script[12] = f\"%let interval_seconds = {sample_interval_s};\\n\"\n",
    "    base_script[13] = f\"%let date_ymd_arg = {date_ymd};\\n\"\n",
    "    base_script[14] = f\"%let output_file  = '{output_filename}';\\n\"\n",
    "    base_script[15] = f\"%let permno_list  = ({', '.join([str(x) for x in permno_selects])});\\n\"\n",
    "    base_script[16] = f\"%let cusip_list  = ({list_to_str(cusip_selects)});\\n\"\n",
    "    base_script[17] = f\"%let symbol_list  = ({list_to_str(symbol_selects)});\\n\"\n",
    "\n",
    "    # For TAQ Monthly case\n",
    "    base_script[18] = f\"%let date_ym_arg  = {date_ymd[:6]};\\n\"\n",
    "\n",
    "    return base_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder params\n",
    "output_folder = '/scratch/duke/sa400/SemiAddress/data_taq_prices/'\n",
    "scripts_folder = '/scratch/duke/sa400/SemiAddress/scripts/'\n",
    "\n",
    "# TAQ selection params\n",
    "start_time, end_time = '9:30:00', '16:05:00'\n",
    "sample_interval_s = 5*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clean up scripts folder\n",
    "files = glob.glob(f'{scripts_folder}*')\n",
    "with Pool(24) as p:\n",
    "    for _ in tqdm(p.imap_unordered(os.remove, files), total = len(files)):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scripts for Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crsp_stock_selects(crsp_df, symbol_selects):\n",
    "    # Grab permnos and cusips for stocks filtered by marketcap rank for the day\n",
    "    \n",
    "    # Prep CRSP dataframe\n",
    "    crsp_df.columns = [x.lower() for x in crsp_df.columns]\n",
    "    crsp_df['mcap']  = crsp_df['prc']*crsp_df['shrout']\n",
    "    crsp_df['cusip9']  = crsp_df['cusip'].astype(str) + crsp_df['cusip'].astype(str).apply(cusip.calc_check_digit)\n",
    "    crsp_df['permno'] = crsp_df['permno'].astype(int)\n",
    "\n",
    "    # Get stocks\n",
    "    permno_selects = stock_info_df.query('ticker in @symbol_selects').groupby(['permno']).first().index.values.astype(int)\n",
    "    cusip_selects = crsp_df.query('permno in @permno_selects')['cusip9'].astype(str).values\n",
    "    permno_selects = list(permno_selects.astype(int).astype(str))\n",
    "\n",
    "    return permno_selects, cusip_selects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:23<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 251/251 [00:22<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:22<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:22<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 250/250 [00:22<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:22<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:23<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:23<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:23<00:00, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 251/251 [00:23<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:24<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:23<00:00, 10.74it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create batch scripts for running SAS programs\n",
    "\n",
    "# Sample selection params\n",
    "symbol_selects = ['SPY', 'BA', 'OXY', 'RTX', 'RTN', 'SLB', 'SPG', 'BIIB', 'BKNG', 'CHTR', 'GILD', 'FB', 'AAPL', 'AMZN', 'NFLX', 'GOOG', 'UTX'] \\\n",
    "    + ['SPYV', 'VFVA', 'VFMO', 'QUAL', 'MTUM', 'SIZE', 'IWM']\n",
    "permno_selects = stock_info_df.query('ticker in @symbol_selects')['permno'].unique()\n",
    "symbol_selects = list(set(symbol_selects + list(stock_info_df.query('permno in @permno_selects')['ticker'].unique())))\n",
    "\n",
    "arg_dates = list(range(2002, 2021))\n",
    "\n",
    "for arg_date in arg_dates:\n",
    "    \n",
    "    print(f'Processing {arg_date}')\n",
    "    arg_date = str(arg_date)\n",
    "    \n",
    "    # Get list of TAQ and CRSP files for given date\n",
    "    k = len(arg_date)\n",
    "    taq_filtered_dates = [str(x) for x in taq_all_dates if str(x)[:k] == arg_date]\n",
    "    crsp_filtered_dates = [str(x) for x in crsp_dates if str(x)[:k] == arg_date]\n",
    "    shared_dates = [x for x in taq_filtered_dates if x in crsp_filtered_dates]\n",
    "    \n",
    "    # Get base SAS script\n",
    "    if int(str(arg_date)[:4]) <= 2014:\n",
    "        taq_library = 'taq'\n",
    "    else:\n",
    "        taq_library = 'taqmsec'\n",
    "\n",
    "    with open(f'{taq_library}_scrape.sas', 'r') as file:\n",
    "        base_script = file.readlines()\n",
    "\n",
    "    # Prepare batch script text file\n",
    "    sh_file = open(f'{scripts_folder}batch_script_{arg_date}.sh', 'w+')\n",
    "\n",
    "    # Add header to batch script file\n",
    "    sh_file.write(f'''\n",
    "    #!/bin/bash\n",
    "    #$ -N taq_{arg_date}\n",
    "    #$ -cwd\\n\\n''')\n",
    "    \n",
    "    # Generate SAS scripts for each day\n",
    "    def update_sh_file(date_str):\n",
    "        \n",
    "        # Skip if weekend\n",
    "        if pd.to_datetime(date_str).weekday() > 4:\n",
    "            return ''\n",
    "        \n",
    "        # Get CRSP file \n",
    "        try: \n",
    "            date_crsp_file = pd.read_parquet(crsp_folder + date_str + '.parquet',\n",
    "                                           columns = ['prc', 'cusip', 'date', 'permno', 'permco', 'shrout'])\n",
    "        except:\n",
    "            print(date_str)\n",
    "            raise Exception\n",
    "        \n",
    "        # Determine appropriate stocks\n",
    "        permno_selects, cusip_selects = get_crsp_stock_selects(date_crsp_file, symbol_selects)\n",
    "        \n",
    "        if len(permno_selects)*len(cusip_selects) == 0:\n",
    "            print('Missing PERMNOs and CUSIPs for', date_str)\n",
    "            return ''\n",
    "        \n",
    "        # Set up output filename\n",
    "        output_filename = output_folder + f'{date_str}.csv'\n",
    "\n",
    "        # Generate SAS script for data collection\n",
    "        file_lines = gen_sas_script(base_script, date_str, start_time, end_time, sample_interval_s, \n",
    "                                    output_filename, permno_selects, cusip_selects, symbol_selects)\n",
    "\n",
    "        # Write SAS script to file\n",
    "        with open(scripts_folder + f'{date_str}.sas', 'w+') as sas_script_file:\n",
    "            sas_script_file.writelines(file_lines)\n",
    "\n",
    "        # Create SAS script reference to .sh file\n",
    "        sas_reference = f'sas {date_str}.sas \\n'\n",
    "\n",
    "        return sas_reference\n",
    "\n",
    "    with Pool(24) as p:\n",
    "        for sh_command in tqdm(p.imap_unordered(update_sh_file, shared_dates), total = len(shared_dates)):\n",
    "            sh_file.write(sh_command)\n",
    "\n",
    "    sh_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qsub -cwd batch_script_2020.sh\n",
      "qsub -cwd batch_script_2019.sh\n",
      "qsub -cwd batch_script_2018.sh\n",
      "qsub -cwd batch_script_2017.sh\n",
      "qsub -cwd batch_script_2016.sh\n",
      "qsub -cwd batch_script_2015.sh\n",
      "qsub -cwd batch_script_2014.sh\n",
      "qsub -cwd batch_script_2013.sh\n",
      "qsub -cwd batch_script_2012.sh\n",
      "qsub -cwd batch_script_2011.sh\n",
      "qsub -cwd batch_script_2010.sh\n",
      "qsub -cwd batch_script_2009.sh\n",
      "qsub -cwd batch_script_2008.sh\n",
      "qsub -cwd batch_script_2007.sh\n",
      "qsub -cwd batch_script_2006.sh\n",
      "qsub -cwd batch_script_2005.sh\n",
      "qsub -cwd batch_script_2004.sh\n",
      "qsub -cwd batch_script_2003.sh\n",
      "qsub -cwd batch_script_2002.sh\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get list of .sh file commands\n",
    "sh_files = list(np.sort([y for y in [x for x in os.walk(scripts_folder)][0][2] if '.sh' in y and '.sh.' not in y]))[::-1]\n",
    "for f in sh_files:\n",
    "    print('qsub -cwd', f)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_folder = '../../data/taq/prices/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [00:02<00:00, 2317.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clean up new output folder\n",
    "files = glob.glob(f'{new_output_folder}*')\n",
    "for f in tqdm(files):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [00:50<00:00, 95.50it/s] \n"
     ]
    }
   ],
   "source": [
    "def reformat_taq_files(date):\n",
    "    \n",
    "    # Get all files for date\n",
    "    files_date = glob.glob(output_folder + date + '*.csv')\n",
    "    taq_df = pd.concat([pd.read_csv(file) for file in files_date], ignore_index = True).reset_index(drop = True)\n",
    "    \n",
    "    # Clean up and save\n",
    "    taq_df.columns = [x.lower() for x in taq_df.columns]\n",
    "    taq_df[['permno', 'cusip9', 'symbol']] = taq_df['ticker_identifier'].str.split('_', expand = True)\n",
    "    taq_df['permno'] = taq_df['permno'].str.replace('.', '', regex = False)\n",
    "    taq_df = taq_df.astype({x:'category' for x in ['permno', 'cusip9', 'symbol', 'ticker_identifier']})\n",
    "    taq_df = taq_df.sort_index(axis=1)\n",
    "    taq_df.to_parquet(f'{new_output_folder}{date}.parquet')\n",
    "\n",
    "# Get list of existing files\n",
    "taq_files = glob.glob(output_folder + '*.csv')\n",
    "taq_dates = [x.split('/')[-1].split('.')[0].split('_')[0] for x in taq_files]\n",
    "taq_dates = list(set(taq_dates))\n",
    "\n",
    "with Pool(24) as p:\n",
    "    for _ in tqdm(p.imap_unordered(reformat_taq_files, taq_dates), total = len(taq_dates)):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filelocs = glob.glob(output_folder + '*.csv')\n",
    "output_filenames = [x.split('/')[-1].split('.')[0] for x in output_filelocs]\n",
    "output_filedates = [x.split('/')[-1].split('_')[0] for x in output_filelocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years: 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020\n",
      "Suffixes: \n",
      "====================================================================================================\n",
      "[2002]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2003]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2004]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2005]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2006]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  251\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2007]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  251\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2008]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  253\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2009]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2010]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2011]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2012]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  250\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2013]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2014]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2015]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2016]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2017]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  251\n",
      "\tMissing files: \n",
      "\t\t\n",
      "\n",
      "====================================================================================================\n",
      "[2018]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  251\n",
      "\tMissing files: \n",
      "\t\t20181230 (weekend), \n",
      "\n",
      "====================================================================================================\n",
      "[2019]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  252\n",
      "\tMissing files: \n",
      "\t\t20190106 (weekend), 20190810 (weekend), \n",
      "\n",
      "====================================================================================================\n",
      "[2020]\n",
      "====================================================================================================\n",
      "[]\n",
      "\tFiles:  253\n",
      "\tMissing files: \n",
      "\t\t20200307 (weekend), 20200907, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_years = np.sort(list(set([x[:4] for x in output_filenames])))\n",
    "output_suffixes = np.sort(list(set([x[9:] for x in output_filenames])))\n",
    "\n",
    "print('Years: ' + ', '.join(output_years))\n",
    "print('Suffixes: ' + ', '.join(output_suffixes))\n",
    "\n",
    "for year_filter in output_years:\n",
    "    \n",
    "    output_filenames_year = [x for x in output_filenames if x[:4] == year_filter]\n",
    "    print('='*100)\n",
    "    print(f'[{year_filter}]')\n",
    "    print('='*100)\n",
    "    \n",
    "\n",
    "    for suffix_filter in output_suffixes:\n",
    "        print(f'[{suffix_filter}]')\n",
    "        output_filenames_year_suffix = [x for x in output_filenames_year if x[9:] == suffix_filter]\n",
    "        output_filenames_year_suffix_dates = [x[:8] for x in output_filenames_year_suffix]\n",
    "        print('\\tFiles: ', len(output_filenames_year_suffix))\n",
    "        print('\\tMissing files: ', end = '\\n\\t\\t')\n",
    "\n",
    "        for date in np.sort([str(x) for x in taq_all_dates if str(x)[:4] == str(year_filter)]):\n",
    "            if date not in output_filenames_year_suffix_dates:\n",
    "                weekend_label = \" (weekend)\" if pd.to_datetime(date).weekday() > 4 else \"\"\n",
    "                print(f'{date}{weekend_label}', end = ', ')\n",
    "        print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
